{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0fvK3Rgac3X"
   },
   "source": [
    "# PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Jvt_q5jKL2y"
   },
   "source": [
    "## Paper Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nS5vzHkTbVkn"
   },
   "source": [
    "### Prior Reseach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point Cloud data 기반의 3D object detection은 다음과 같은 2가지 접근법으로 나눠져있었음\n",
    "1. Point Cloud를 3D grid voxel로 나누어 **Sparse 3D convolution**을 적용하는 **Grid-based approach**   \n",
    "→ computationally efficient 하지만 kernel size에 따라 receptive field로 인해 localization의 정확도 하락 \n",
    "\n",
    "2. Point Cloud를 raw하게 **PointNet set abstraction**으로 representation을 얻어 적용하는 **Point-based approach**  \n",
    "→ tight한 localization이 가능하게 되지만 point-wise로 계산된다는 측면에서 computationally expensive\n",
    "\n",
    "∴ 각각의 advantage를 동시에 갖을 수 있도록 **두 approach를 적절히 결합**한 **2-staged detector**인 **PV-RCNN** 를 제안   \n",
    "→ Gird-based approach로 RPN을 수행하여 bounding box를 얻고 해당 bounding box에서 Point-based approach로 boundary refinement\n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"959\" alt=\"1\" src=\"https://user-images.githubusercontent.com/86907286/201465648-a36e6266-d566-4bf1-a42c-d314a9595445.png\">\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Voxel CNN for Efficient Feature Encoding and Proposal Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid-based approach로서 input space를 voxel grid로 나누고 **Sparse 3D convolution**을 적용하는 efficient backbone이 제시되어 있음  \n",
    "traditional convolution이 im2col로 연산되는 것과 달리 input/output valid point를 각각 **hash table로 저장**하고 **Rulebook**으로 연산 관계를 정의  \n",
    "→ on-the-fly로 필요한 연산만을 수행할 수 있으므로 sparse data에 해당하는 Point Cloud에서 efficient backbone이 될 수 있음\n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"695\" alt=\"2\" src=\"https://user-images.githubusercontent.com/86907286/201465651-37349412-77ef-44fe-b810-fe6345924b95.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 효율적인 Sparse 3D convolution으로 3D volume feature 또는 2D BEV representation을 통해서 적절한 Anchor를 통해서 RPN 구성 가능  \n",
    "그러나 이는 feature volume이 down-sampling 되므로 output volume의 spatial resolution이 너무 낮아져 object localization이 정확하지 못함  \n",
    "→ 정확도를 올리기 위해 up-sampling으로 다시 되돌리면 sparse해지므로 computationally expensive해지는 문제가 발생\n",
    "\n",
    "∴ Sparse 3D convolution backbone을 사용하면서 정확도가 손실된 region proposal을 **refine**하는 추가적인 architecture가 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voxel-to-keypoint Scene Encoding via Voxel Set Abstraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backbone에서 얻어지는 region proposal을 refine하기 위해 PoinNet의 **Set Abstraction**을 활용할 수 있음    \n",
    "이를 통해 **proposal region 내부의 voxel에서의 keypoint**를 중심으로 non-empty voxel 내부의 semantic catch 가능  \n",
    "→ 해당 keypoint들과 backbone에서 얻어지는 voxel set을 참고하여 **ROI-grid pooling**을 수행하여 bounding box refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keypoint sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point Cloud는 기본적으로 sparse하고 irregular하기 때문에 input을 raw하게 다루는데에 문제가 생김  \n",
    "이를 위해서 FPS(Farthest Point Sampling)을 통해서 point를 얻어 최대한 uniform하게 참조할 수 있도록 **keypoint**를 만들어야 함  \n",
    "→ 이는 **empty voxel이 아닌 grid에서 point가 uniform해지므로** backbone feature를 참조할 때 좋은 point subset이 될 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voxel Set Abstraction Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "raw keypoint만을 통해 Set Abstraction을 진행하여 semantic feature을 얻기 위해서는 receptive field 측면에서 부족한 점이 많음  \n",
    "따라서 backbone에서 얻어지는 voxel-wise feature를 neighborhood로 동시에 사용하여 **multi-scale semantic feature**를 얻어내는 방법을 선택 가능  \n",
    "→ $k$-th level에서의 $n$ keypoints $\\mathcal{K} = \\{p_i\\}$ 마다의 voxel neighboorhood set $\\mathcal{S_i}^{(l_k)}$에도 Set Abstraction을 추가적으로 적용해 concatenate  \n",
    "\n",
    "$$ S_i^{(l_k)} = \\{[f^{(l_k)}_j; v^{(l_k)}_j - p_i]^T | \\|v^{(l_k)}_j - p_i\\|^2 < r_k, \\forall f^{(l_k)}_j \\in \\mathcal{F}^{(l_k)},\\forall v^{(l_k)}_j \\in \\mathcal{V}^{(l_k)} \\}$$\n",
    "\n",
    "이때 $\\mathcal{F}^{(l_k)} = \\{f^{(l_k)}_j\\}$는 $k$-th level feature vector이며 해당 feature vector의 original space coordinate는 $\\mathcal{V}^{(l_k)} = \\{v^{(l_k)}_j\\}$ 로 표현   \n",
    "$k$-th level에서의 neighborhood radius를 결정하는 $r_k$는 다양한 receptive field를 얻기 위해 $k$-th level마다 2종류씩 사용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 얻은 voxel neighboorhood set $S_i^{(l_k)}$를 Set Abstraction 방식을 통해 key point $p_i$를 표현하는 feature vector $f_i^{(pv_k)}$를 얻음  \n",
    "이때 $S_i^{(l_k)}$에서도 computational efficiency를 위해 최대 $T_k$의 voxel만큼 random sampling하는 $\\mathcal{M}(\\cdot)$을 적용한 후 MLP $G(\\cdot)$, max pooling을 적용\n",
    "\n",
    "$$ f_i^{(pv_k)} = \\max \\{ G(\\mathcal{M}(S_i^{(l_k)})) \\}$$\n",
    "\n",
    "최종적으로 모든 $k$-th level, 제안된 Architecture로는 4단계의 $f_i^{(pv_k)}$를 concatenate하여 keypoint $p_i$에 대한 multi-scale semantic feature로 사용\n",
    "\n",
    "$$ f_i^{(pv)} = [f_i^{(pv_1)}, f_i^{(pv_2)}, f_i^{(pv_3)}, f_i^{(pv_4)}] \\text{ for } i = 1, 2, \\cdots, n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extended VSA Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voxel Set Abstraction layer를 통해서 얻어진 feature vector를 그대로 사용하지 않고 추가적인 feature로 $f_i^{(raw)}$와 $f_i^{(bev)}$를 결합  \n",
    "$f_i^{(raw)}$는 raw Point Cloud에서 keypoint에 대해 얻어진 feature로서 voxelization으로 발생한 quantization을 보상해주기 위함   \n",
    "$f_i^{(bev)}$와 bilinear interpolation으로 backbone을 통해 얻어진 keypoint에 대해 down-sampled 2D BEV feature로서 overall semantic을 더해줌  \n",
    "\n",
    "$$ f_i^{(p)} = [f_i^{(pv)}, f_i^{(raw)}, f_i^{(bev)}] \\text{ for } i = 1, 2, \\cdots, n $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicted Keypoint Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞선 과정에서 keypoint들은 FPS로 sampling되었기 때문에 실제로 관심을 갖는 object보다 **background**를 나타내는 point가 많게 됨  \n",
    "그러나 insight 측면에서 refinement에서 background는 그렇게 중요하지 않기 때문에 **foreground point가 더 contribution을 하도록** weighting이 필요  \n",
    "→ 이를 ground-truth bounding box를 통해 Point Cloud의 segmentation label을 만들어 supervision을 통해 weighted feature $\\tilde{f}_i^{(p)}$를 사용\n",
    "\n",
    "ground-truth bounding box가 있다면 해당 box에 각각의 keypoint가 들어가있는지 아닌지에 대한 binary classification label을 얻을 수 있음  \n",
    "이를 통해 적절한 MLP $\\mathcal{A}(\\cdot)$를 통해 주어진 keypoint feature의 weight를 예측하도록 구성하여 focal loss를 통해서 학습\n",
    "\n",
    "$$ \\tilde{f}_i^{(p)} = \\mathcal{A}(f_i^{(p)}) \\cdot f_i^{(p)} $$\n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"460\" alt=\"3\" src=\"https://user-images.githubusercontent.com/86907286/201465654-36b8e618-fc2c-462a-b1bd-28325c662e19.png\">\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keypoint-to-grid RoI Feature Abstraction for Proposal Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoI-grid Pooling via Set Abstraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VSA module을 통해 얻어진 것은 일부의 3D multi-scale feature keypoint features $\\mathcal{F}=\\{\\tilde{f}_1^{(p)}, \\cdots, \\tilde{f}_n^{(p)} \\}$ 라고 할 수 있음  \n",
    "따라서 이를 활용하여 3D proposal을 robust하고 accurate하게 refine하기 위해 $\\mathcal{F}$의 semantic을 region proposal에 반영하는 **RoI-grid pooling**이 추가적으로 필요    \n",
    "$6 \\times 6 \\times 6$으로 3D proposal 내부의 point를 uniform sampling하여 $\\mathcal{G} = \\{ g_1, \\cdots, g_{216} \\}$를 얻고 이들을 keypoint와 비교하는 것으로 Set Abstraction 수행  \n",
    "이때 Set Abstraction을 사용하기 위한 keypoint neighborhood set $\\tilde{\\Psi}$을 VSA module로 얻은 feature와 keypoint와의 relative distance를 concatenate하여 표현  \n",
    "\n",
    "$$ \\tilde{\\Psi} = \\{[\\tilde{f}_j^{(p)}; p_j - g_i]^T | \\|p_j - g_i\\|^2 < \\tilde{r}, \\forall p_j \\in \\mathcal{K},\\forall \\tilde{f}_j^{(p)} \\in \\tilde{\\mathcal{F}} \\} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 얻은 $\\tilde{\\Psi}$를 앞선 Voxel Set Abstraction과 유사하게 Set Abstraction을 수행하여 **grid sample point $g_i$에 대한 feature**를 얻음  \n",
    "마찬가지로 다양한 receptive field를 얻기 위해 neighborhood radii $\\tilde{r}$은 2종류를 선택하여 구현되었음  \n",
    "\n",
    "$$ \\tilde{f}_i^{(g)} = \\max \\{ G(\\mathcal{M}(\\tilde{\\Psi})) \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 같은 RoI에 존재하는 모든 grid sample마다 얻어진 feature는 모아져 256 feature dimension output의 two-layer MLP를 통해 RoI-feature로 transform됨  \n",
    "이는 receptive field를 통해 얻어지는 context라 **RoI boundary에 존재하는 keypoint가 boundary를 넘어서까지의 context**를 볼 수 있도록 도움  \n",
    "boundary의 정보는 **boundary를 기준으로 안과 밖의 context를 비교하는 중요하므로** average를 하거나 uninformative한 정보를 제외하는 것과 다른 장점이 있음  \n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"462\" alt=\"4\" src=\"https://user-images.githubusercontent.com/86907286/201465658-9acf9330-8cc3-4585-91dc-418b7b23db48.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3D Proposal Refinement and Confidence Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proposal region에 대한 RoI feature를 통해 실제로 Voxel CNN backbone에서 얻어진 proposal region을 refine하기 위한 ground-truth와의 잔차를 학습시킬 필요가 있음  \n",
    "이를 위해 2개의 branch로 나누어진 two-layer MLP를 사용해 confidence prediction과 box refinement branch로 나누어 추가적인 head를 구성  \n",
    "confidence prediction branch는 ground-truth box와 proposal box와의 $k$-th 3D IoU를 통해 confidence target $y_k \\in [0, 1]$를 얻어 binary-cross entropy로 학습  \n",
    "\n",
    "$$ y_k = \\min (1, \\max (0, 2\\text{IoU}_k - 0.5)) $$\n",
    "$$ \\mathcal{L}_{\\text{iou}} = -y_k \\log (\\tilde{y}_k) - (1 - y_k) \\log (\\tilde{y}_k) $$\n",
    "\n",
    "box refinement branch는 일반적인 box regressor처럼 smooth-L1 loss를 통해서 box residual를 예측하도록 학습 \n",
    "\n",
    "$$ \\text{smooth}_{L_1}(x) = \\begin{cases}\n",
    "  0.5x^2 & \\text{if } |x| < 1, \\\\\n",
    "  |x|-0.5 & \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종적인 training loss $\\mathcal{L}$은 backbone에서 바로 얻어지는 region proposal loss $\\mathcal{L_{\\text{rpn}}}$, keypoint segmentation loss $\\mathcal{L}_{\\text{seg}}$, proposal refinement loss $\\mathcal{L}_{\\text{rcnn}}$로 구성됨  \n",
    "\n",
    "$$ \\mathcal{L} = \\mathcal{L_{\\text{rpn}}} + \\mathcal{L}_{\\text{seg}} + \\mathcal{L}_{\\text{rcnn}} $$ \n",
    "\n",
    "$\\mathcal{L_{\\text{rpn}}}$은 focal loss로 계산되는 $\\mathcal{L_{\\text{cls}}}$과 anchor box와 ground-truth의 residual $\\hat{\\Delta \\text{r}^a}$을 예측하는 smooth-L1 loss $\\mathcal{L_{\\text{smooth-L1}}}$로 구성됨\n",
    "\n",
    "$$ \\mathcal{L_{\\text{rpn}}} = \\mathcal{L_{\\text{cls}}} + \\beta \\sum_{\\text{r} \\in \\{x, y, z, l, h, w, \\theta\\}} \\mathcal{L_{\\text{smooth-L1}}} (\\hat{\\Delta \\text{r}^a}, \\Delta \\text{r}^a) $$\n",
    "\n",
    "$\\mathcal{L}_{\\text{seg}}$는 ground-truth bounding box 안에 있는지 없는지에 대한 binary classification loss로 표현됨  \n",
    "$\\mathcal{L}_{\\text{rcnn}}$는 $\\mathcal{L}_{\\text{iou}}$와 함께 anchor box를 통해 proposed된 predicted box와 ground truth box와의 residual $\\hat{\\Delta \\text{r}^p} = \\hat{\\Delta \\text{r}^a}$에 대한 smooth-L1 loss $\\mathcal{L_{\\text{smooth-L1}}}$로 구성\n",
    "\n",
    "$$ \\mathcal{L}_{\\text{rcnn}} = \\mathcal{L_{\\text{cls}}} + \\beta \\sum_{\\text{r} \\in \\{x, y, z, l, h, w, \\theta\\}} \\mathcal{L_{\\text{smooth-L1}}} (\\hat{\\Delta \\text{r}^p}, \\Delta \\text{r}^p) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablation Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제안된 module들에 대한 ablation study로 다음과 같은 장점들을 확인할 수 있음\n",
    "\n",
    "1. voxel-to-keypoint scene encoding  \n",
    "→ 3D voxel CNN에 추가적인 keypoint 정보를 통한 segmentation supervision을 multi-scale learning 측면에서 좋은 효과를 주는 것을 확인 가능\n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"465\" alt=\"5\" src=\"https://user-images.githubusercontent.com/86907286/201465659-0277f7e8-a7eb-4f9b-97a2-bb4a85104a4c.png\">\n",
    "</p>\n",
    "\n",
    "\n",
    "2. Voxel Set Abstraction module   \n",
    "→ 단순히 raw point feature $f_i^{(raw)}$만을 사용하는 것보다 $f_i^{(pv_k)}$를 같이 사용하는 것이 성능 향상폭에는 차이가 있지만 도움을 준다는 것을 확인 가능 \n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"465\" alt=\"6\" src=\"https://user-images.githubusercontent.com/86907286/201465661-98dbea31-c358-4124-a12f-1fe378309075.png\">\n",
    "</p>\n",
    "\n",
    "\n",
    "3. Predicted Keypoint Weighting module   \n",
    "→ 적용한 것과 적용하지 않은 것의 경우 성능 차이가 많이 나므로 multi-scale feature aggregation 측면에서 foreground/background를 구분하는게 중요하다는 것을 알 수 있음\n",
    "\n",
    "4. RoI-grid pooling module  \n",
    "→ 기존의 approach를 사용하는 경우보다 Moderate, Hard case에서 성능의 차이가 나며, 특히 IoU를 통해 confidence target을 설정하는게 효과적임을 확인 가능\n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"462\" alt=\"7\" src=\"https://user-images.githubusercontent.com/86907286/201465662-401fb979-b871-4c9a-aeb0-41274e24b7ea.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxPFMMWQKOEY"
   },
   "source": [
    "## Implementation Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseCNNBase(nn.Module):\n",
    "    \"\"\"\n",
    "    block      shape    stride\n",
    "    0    [ 4, 8y, 8x, 41]    1\n",
    "    1    [32, 4y, 4x, 21]    2\n",
    "    2    [64, 2y, 2x, 11]    4\n",
    "    3    [64, 1y, 1x,  5]    8\n",
    "    4    [64, 1y, 1x,  2]    8\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        \"\"\"grid_shape given in ZYX order.\"\"\"\n",
    "        super(SparseCNNBase, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.grid_shape = compute_grid_shape(cfg)\n",
    "        self.base_voxel_size = torch.cuda.FloatTensor(cfg.VOXEL_SIZE)\n",
    "        self.voxel_offset = torch.cuda.FloatTensor(cfg.GRID_BOUNDS[:3])\n",
    "        self.make_blocks(cfg)\n",
    "\n",
    "    def make_blocks(self, cfg):\n",
    "        \"\"\"Subclasses must implement this method.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def maybe_bias_init(self, module, val):\n",
    "        if hasattr(module, \"bias\") and module.bias is not None:\n",
    "            nn.init.constant_(module.bias, val)\n",
    "\n",
    "    def kaiming_init(self, module):\n",
    "        nn.init.kaiming_normal_(\n",
    "            module.weight, a=0, mode='fan_out', nonlinearity='relu')\n",
    "        self.maybe_bias_init(module, 0)\n",
    "\n",
    "    def batchnorm_init(self, module):\n",
    "        nn.init.constant_(module.weight, 1)\n",
    "        self.maybe_bias_init(module, 0)\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                self.kaiming_init(m)\n",
    "            elif isinstance(m, _BatchNorm):\n",
    "                self.batchnorm_init(m)\n",
    "\n",
    "    def to_global(self, stride, volume):\n",
    "        \"\"\"\n",
    "        Convert integer voxel indices to metric coordinates.\n",
    "        Indices are reversed ijk -> kji to maintain correspondence with xyz.\n",
    "        Sparse voxels are padded with subsamples to allow batch PointNet processing.\n",
    "        :voxel_size length-3 tensor describing size of atomic voxel, accounting for stride.\n",
    "        :voxel_offset length-3 tensor describing coordinate offset of voxel grid.\n",
    "        \"\"\"\n",
    "        index = torch.flip(volume.indices, (1,))\n",
    "        voxel_size = self.base_voxel_size * stride\n",
    "        xyz = index[..., 0:3].float() * voxel_size\n",
    "        xyz = (xyz + self.voxel_offset)\n",
    "        xyz = self.pad_batch(xyz, index[..., -1], volume.batch_size)\n",
    "        feature = self.pad_batch(volume.features, index[..., -1], volume.batch_size)\n",
    "        return xyz, feature\n",
    "\n",
    "    def compute_pad_amounts(self, batch_index, batch_size):\n",
    "        \"\"\"Compute padding needed to form dense minibatch.\"\"\"\n",
    "        helper_index = torch.arange(batch_size + 1, device=batch_index.device)\n",
    "        helper_index = helper_index.unsqueeze(0).contiguous().int()\n",
    "        batch_index = batch_index.unsqueeze(0).contiguous().int()\n",
    "        start_index = searchsorted(batch_index, helper_index).squeeze(0)\n",
    "        batch_count = start_index[1:] - start_index[:-1]\n",
    "        pad = list((batch_count.max() - batch_count).cpu().numpy())\n",
    "        batch_count = list(batch_count.cpu().numpy())\n",
    "        return batch_count, pad\n",
    "\n",
    "    def pad_batch(self, x, batch_index, batch_size):\n",
    "        \"\"\"Pad sparse tensor with subsamples to form dense minibatch.\"\"\"\n",
    "        if batch_size == 1:\n",
    "            return x.unsqueeze(0)\n",
    "        batch_count, pad = self.compute_pad_amounts(batch_index, batch_size)\n",
    "        chunks = x.split(batch_count)\n",
    "        pad_values = [random_choice(c, n) for (c, n) in zip(chunks, pad)]\n",
    "        chunks = [torch.cat((c, p)) for (c, p) in zip(chunks, pad_values)]\n",
    "        return torch.stack(chunks)\n",
    "\n",
    "    def to_bev(self, volume):\n",
    "        \"\"\"Collapse z-dimension to form BEV feature map.\"\"\"\n",
    "        volume = volume.dense()\n",
    "        N, C, D, H, W = volume.shape\n",
    "        bev = volume.view(N, C * D, H, W)\n",
    "        return bev\n",
    "\n",
    "    def forward(self, features, coordinates, batch_size):\n",
    "        x0 = spconv.SparseConvTensor(\n",
    "            features, coordinates.int(), self.grid_shape, batch_size\n",
    "        )\n",
    "        x1 = self.blocks[0](x0)\n",
    "        x2 = self.blocks[1](x1)\n",
    "        x3 = self.blocks[2](x2)\n",
    "        x4 = self.blocks[3](x3)\n",
    "        x4 = self.to_bev(x4)\n",
    "        args = zip(self.cfg.STRIDES, (x0, x1, x2, x3))\n",
    "        x = list(itertools.starmap(self.to_global, args))\n",
    "        return x, x4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposal head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProposalLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Use BEV feature map to generate 3D box proposals.\n",
    "    TODO: Fix long variable names, ugly line wraps.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super(ProposalLayer, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.conv_cls = nn.Conv2d(\n",
    "            cfg.PROPOSAL.C_IN, cfg.NUM_CLASSES * cfg.NUM_YAW, 1)\n",
    "        self.conv_reg = nn.Conv2d(\n",
    "            cfg.PROPOSAL.C_IN, cfg.NUM_CLASSES * cfg.NUM_YAW * cfg.BOX_DOF, 1)\n",
    "        self.TOPK, self.DOF = cfg.PROPOSAL.TOPK, cfg.BOX_DOF\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.constant_(self.conv_cls.bias, (-math.log(1 - .01) / .01))\n",
    "        nn.init.constant_(self.conv_reg.bias, 0)\n",
    "        for m in (self.conv_cls.weight, self.conv_reg.weight):\n",
    "            nn.init.normal_(m, std=0.01)\n",
    "\n",
    "    def _generate_group_idx(self, B, n_cls):\n",
    "        \"\"\"Compute unique group_idx based on (batch_idx, class_idx) tuples.\"\"\"\n",
    "        batch_idx = torch.arange(B)[:, None].expand(-1, n_cls)\n",
    "        class_idx = torch.arange(n_cls)[None, :].expand(B, -1)\n",
    "        group_idx = class_idx + n_cls * batch_idx\n",
    "        b, c, g = [x[..., None].expand(-1, -1, self.TOPK).reshape(-1)\n",
    "            for x in (batch_idx, class_idx, group_idx)]\n",
    "        return b, c, g\n",
    "\n",
    "    def _above_score_thresh(self, scores, class_idx):\n",
    "        \"\"\"Classes may have different score thresholds.\"\"\"\n",
    "        thresh = scores.new_tensor([a['score_thresh'] for a in self.cfg.ANCHORS])\n",
    "        mask = scores > thresh[class_idx]\n",
    "        return mask\n",
    "\n",
    "    def _multiclass_batch_nms(self, boxes, scores):\n",
    "        \"\"\"Only boxes with same group_idx are jointly considered in nms\"\"\"\n",
    "        B, n_cls = scores.shape[:2]\n",
    "        scores = scores.view(-1)\n",
    "        boxes = boxes.view(-1, self.DOF)\n",
    "        bev_boxes = boxes[:, [0, 1, 3, 4, 6]]\n",
    "        batch_idx, class_idx, group_idx = self._generate_group_idx(B, n_cls)\n",
    "        idx = batched_nms_rotated(bev_boxes, scores, group_idx, iou_threshold=0.01)\n",
    "        boxes, batch_idx, class_idx, scores = \\\n",
    "            [x[idx] for x in (boxes, batch_idx, class_idx, scores)]\n",
    "        mask = self._above_score_thresh(scores, class_idx)\n",
    "        out = [x[mask] for x in (boxes, batch_idx, class_idx, scores)]\n",
    "        return out\n",
    "\n",
    "    def _decode(self, reg_map, anchors, anchor_idx):\n",
    "        \"\"\"Expands anchors in batch dimension and calls decode.\"\"\"\n",
    "        B, n_cls = reg_map.shape[:2]\n",
    "        anchor_idx = anchor_idx[..., None].expand(-1, -1, -1, self.DOF)\n",
    "        deltas = reg_map.reshape(B, n_cls, -1, self.cfg.BOX_DOF) \\\n",
    "            .gather(2, anchor_idx)\n",
    "        anchors = anchors.view(1, n_cls, -1, self.cfg.BOX_DOF) \\\n",
    "            .expand(B, -1, -1, -1).gather(2, anchor_idx)\n",
    "        boxes = decode(deltas, anchors)\n",
    "        return boxes\n",
    "\n",
    "    def inference(self, feature_map, anchors):\n",
    "        \"\"\":return (boxes, batch_idx, class_idx, scores)\"\"\"\n",
    "        cls_map, reg_map = self(feature_map)\n",
    "        score_map = cls_map.sigmoid_()\n",
    "        B, n_cls = score_map.shape[:2]\n",
    "        scores, anchor_idx = score_map.view(B, n_cls, -1).topk(self.TOPK, -1)\n",
    "        boxes = self._decode(reg_map, anchors, anchor_idx)\n",
    "        out = self._multiclass_batch_nms(boxes, scores)\n",
    "        return out\n",
    "\n",
    "    def reshape_cls(self, cls_map):\n",
    "        B, _, ny, nx = cls_map.shape\n",
    "        shape = (B, self.cfg.NUM_CLASSES, self.cfg.NUM_YAW, ny, nx)\n",
    "        cls_map = cls_map.view(shape)\n",
    "        return cls_map\n",
    "\n",
    "    def reshape_reg(self, reg_map):\n",
    "        B, _, ny, nx = reg_map.shape\n",
    "        shape = (B, self.cfg.NUM_CLASSES, self.cfg.BOX_DOF, -1, ny, nx)\n",
    "        reg_map = reg_map.view(shape).permute(0, 1, 3, 4, 5, 2)\n",
    "        return reg_map\n",
    "\n",
    "    def forward(self, feature_map):\n",
    "        cls_map = self.reshape_cls(self.conv_cls(feature_map))\n",
    "        reg_map = self.reshape_reg(self.conv_reg(feature_map))\n",
    "        return cls_map, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refinement head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefinementLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Uses pooled features to refine proposals.\n",
    "    TODO: Pass class predictions from proposals since this\n",
    "        module only predicts confidence.\n",
    "    TODO: Implement RefinementLoss.\n",
    "    TODO: Decide if decode box predictions / apply box\n",
    "        deltas here or elsewhere.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super(RefinementLayer, self).__init__()\n",
    "        self.mlp = self.build_mlp(cfg)\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def build_mlp(self, cfg):\n",
    "        \"\"\"\n",
    "        TODO: Check if should use bias.\n",
    "        \"\"\"\n",
    "        channels = cfg.REFINEMENT.MLPS + [cfg.BOX_DOF + 1]\n",
    "        mlp = MLP(channels, bias=True, bn=False, relu=[True, False])\n",
    "        return mlp\n",
    "\n",
    "    def apply_refinements(self, box_deltas, boxes):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inference(self, points, features, boxes):\n",
    "        box_deltas, scores = self(points, features, boxes)\n",
    "        boxes = self.apply_refinements(box_deltas, boxes)\n",
    "        scores = scores.sigmoid()\n",
    "        positive = 1 - scores[..., -1:]\n",
    "        _, indices = torch.topk(positive, k=self.cfg.PROPOSAL.TOPK, dim=1)\n",
    "        indices = indices.expand(-1, -1, self.cfg.NUM_CLASSES)\n",
    "        box_indices = indices[..., None].expand(-1, -1, -1, self.cfg.BOX_DOF)\n",
    "        scores = scores.gather(1, indices)\n",
    "        boxes = boxes.gather(1, box_indices)\n",
    "        return boxes, scores, indices\n",
    "\n",
    "    def forward(self, points, features, boxes):\n",
    "        refinements = self.mlp(features.permute(0, 2, 1))\n",
    "        box_deltas, scores = refinements.split(1)\n",
    "        return box_deltas, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoI-grid pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoiGridPool(nn.Module):\n",
    "    \"\"\"\n",
    "    Pools features from within proposals.\n",
    "    TODO: I think must be misunderstanding dimensions claimed in paper.\n",
    "        If sample 216 gridpoints in each proposal, and keypoint features\n",
    "        are of dim 256, and gridpoint features are vectorized before linear layer,\n",
    "        causes 216 * 256 * 256 parameters in reduction...\n",
    "    TODO: Document input and output sizes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super(RoiGridPool, self).__init__()\n",
    "        self.pnet = self.build_pointnet(cfg)\n",
    "        self.reduction = MLP(cfg.GRIDPOOL.MLPS_REDUCTION)\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def build_pointnet(self, cfg):\n",
    "        \"\"\"Copy channel list because PointNet modifies it in-place.\"\"\"\n",
    "        pnet = PointnetSAModuleMSG(\n",
    "            npoint=-1, radii=cfg.GRIDPOOL.RADII_PN,\n",
    "            nsamples=cfg.SAMPLES_PN,\n",
    "            mlps=deepcopy(cfg.GRIDPOOL.MLPS_PN), use_xyz=True,\n",
    "        )\n",
    "        return pnet\n",
    "\n",
    "    def rotate_z(self, points, theta):\n",
    "        \"\"\"\n",
    "        Rotate points by theta around z-axis.\n",
    "        :points (b, n, m, 3)\n",
    "        :theta (b, n)\n",
    "        :return (b, n, m, 3)\n",
    "        \"\"\"\n",
    "        b, n, m, _ = points.shape\n",
    "        theta = theta.unsqueeze(-1).expand(-1, -1, m)\n",
    "        xy, z = torch.split(points, [2, 1], dim=-1)\n",
    "        c, s = torch.cos(theta), torch.sin(theta)\n",
    "        R = torch.stack((c, -s, s, c), dim=-1).view(b, n, m, 2, 2)\n",
    "        xy = torch.matmul(R, xy.unsqueeze(-1))\n",
    "        xyz = torch.cat((xy.squeeze(-1), z), dim=-1)\n",
    "        return xyz\n",
    "\n",
    "    def sample_gridpoints(self, boxes):\n",
    "        \"\"\"\n",
    "        Sample axis-aligned points, then rotate.\n",
    "        :return (b, n, ng, 3)\n",
    "        \"\"\"\n",
    "        b, n, _ = boxes.shape\n",
    "        m = self.cfg.GRIDPOOL.NUM_GRIDPOINTS\n",
    "        gridpoints = boxes[:, :, None, 3:6] * \\\n",
    "            (torch.rand((b, n, m, 3), device=boxes.device) - 0.5)\n",
    "        gridpoints = boxes[:, :, None, 0:3] + \\\n",
    "            self.rotate_z(gridpoints, boxes[..., -1])\n",
    "        return gridpoints\n",
    "\n",
    "    def forward(self, proposals, keypoint_xyz, keypoint_features):\n",
    "        b, n, _ = proposals.shape\n",
    "        m = self.cfg.GRIDPOOL.NUM_GRIDPOINTS\n",
    "        gridpoints = self.sample_gridpoints(proposals).view(b, -1, 3)\n",
    "        features = self.pnet(keypoint_xyz, keypoint_features, gridpoints)[1]\n",
    "        features = features.view(b, -1, n, m) \\\n",
    "            .permute(0, 2, 1, 3).contiguous().view(b, n, -1)\n",
    "        features = self.reduction(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PV_RCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    TODO: Improve docstrings.\n",
    "    TODO: Some docstrings may claim incorrect dimensions.\n",
    "    TODO: Figure out clean way to handle proposals_only forward.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super(PV_RCNN, self).__init__()\n",
    "        self.pnets = self.build_pointnets(cfg)\n",
    "        self.roi_grid_pool = RoiGridPool(cfg)\n",
    "        self.vfe = VoxelFeatureExtractor()\n",
    "        self.cnn = CNN_FACTORY[cfg.CNN](cfg)\n",
    "        self.bev = BEVFeatureGatherer(\n",
    "            cfg, self.cnn.voxel_offset, self.cnn.base_voxel_size)\n",
    "        self.proposal_layer = ProposalLayer(cfg)\n",
    "        self.refinement_layer = RefinementLayer(cfg)\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def build_pointnets(self, cfg):\n",
    "        \"\"\"Copy list because PointNet modifies it in-place.\"\"\"\n",
    "        pnets = []\n",
    "        for i, mlps in enumerate(cfg.PSA.MLPS):\n",
    "            pnets += [PointnetSAModuleMSG(\n",
    "                npoint=-1, radii=cfg.PSA.RADII[i],\n",
    "                nsamples=cfg.SAMPLES_PN,\n",
    "                mlps=deepcopy(mlps), use_xyz=True,\n",
    "            )]\n",
    "        return nn.Sequential(*pnets)\n",
    "\n",
    "    def sample_keypoints(self, points):\n",
    "        \"\"\"\n",
    "        fps expects points shape (B, N, 3)\n",
    "        fps returns indices shape (B, K)\n",
    "        gather expects features shape (B, C, N)\n",
    "        \"\"\"\n",
    "        points = points[..., :3].contiguous()\n",
    "        indices = furthest_point_sample(points, self.cfg.NUM_KEYPOINTS)\n",
    "        keypoints = gather_operation(points.transpose(1, 2).contiguous(), indices)\n",
    "        keypoints = keypoints.transpose(1, 2).contiguous()\n",
    "        return keypoints\n",
    "\n",
    "    def _pointnets(self, cnn_out, keypoint_xyz):\n",
    "        \"\"\"xyz (B, N, 3) | features (B, N, C) | new_xyz (B, M, C) | return (B, M, Co)\"\"\"\n",
    "        pnet_out = []\n",
    "        for (voxel_xyz, voxel_features), pnet in zip(cnn_out, self.pnets):\n",
    "            voxel_xyz = voxel_xyz.contiguous()\n",
    "            voxel_features = voxel_features.transpose(1, 2).contiguous()\n",
    "            out = pnet(voxel_xyz, voxel_features, keypoint_xyz)[1]\n",
    "            pnet_out += [out]\n",
    "        return pnet_out\n",
    "\n",
    "    def point_feature_extract(self, item, cnn_features, bev_map):\n",
    "        points_split = torch.split(item['points'], [3, 1], dim=-1)\n",
    "        cnn_features = [points_split] + cnn_features\n",
    "        point_features = self._pointnets(cnn_features, item['keypoints'])\n",
    "        bev_features = self.bev(bev_map, item['keypoints'])\n",
    "        point_features = torch.cat(point_features + [bev_features], dim=1)\n",
    "        return point_features\n",
    "\n",
    "    def proposal(self, item):\n",
    "        item['keypoints'] = self.sample_keypoints(item['points'])\n",
    "        features = self.vfe(item['features'], item['occupancy'])\n",
    "        cnn_features, bev_map = self.cnn(features, item['coordinates'], item['batch_size'])\n",
    "        scores, boxes = self.proposal_layer(bev_map)\n",
    "        item.update(dict(P_cls=scores, P_reg=boxes))\n",
    "        return item\n",
    "\n",
    "    def forward(self, item):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ak0Dn-LURm2"
   },
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahcpUlB9UUpB"
   },
   "source": [
    "- https://arxiv.org/abs/1912.13192\n",
    "- https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1  \n",
    "- https://github.com/jhultman/vision3d/tree/master/vision3d/detector"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOy6sMktL3Wiz8Wfsn1WpYv",
   "collapsed_sections": [
    "uxPFMMWQKOEY",
    "-w2YtibfT8Zy",
    "5g9CBFXgUuNR",
    "7hkeM1CRU3W0"
   ],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
